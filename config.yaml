# ===========================
# 系统设置：与服务器初始化相关的设置
system_config:
  host: "localhost" # 服务器监听的地址，"0.0.0.0" 表示监听所有网络接口；如果需要安全，可以使用 "127.0.0.1"（仅本地访问）
  port: 29999 # 服务器监听的端口

# === 自动语音识别 ===
asr_config:
  # 语音转文本模型选项："faster_whisper", "whisper_cpp", "whisper", "fun_asr", "sherpa_onnx_asr"
  # 使用的语音识别模型
  asr_model: "sherpa_onnx_asr"

  # Faster Whisper 配置
  faster_whisper:
    # https://huggingface.co/Systran
    # "distil-large-v3", "large-v3"
    model_path: "large-v3"
    # 模型下载根目录, 启动时模型会自动下载到该目录
    download_root: "models/whisper"
    # 语言 af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh, yue
    # 留空表示自动检测。
    language: "zh"
    # 设备，cpu、cuda 或 auto。faster-whisper 不支持 mps
    device: "auto"

  whisper_cpp:
    # https://absadiki.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
    # GGML_CUDA=1 pip install git+https://github.com/absadiki/pywhispercpp
    # WHISPER_COREML=1 pip install git+https://github.com/absadiki/pywhispercpp
    model_name: "large-v3" # 模型名称
    model_dir: "models/whisper" # 模型目录
    print_realtime: True # 是否实时打印
    print_progress: True # 是否打印进度
    language: "zh" # 语言，en、zh、auto

  whisper:
    # https://github.com/openai/whisper/blob/main/model-card.md
    name: "large" # 模型名称
    download_root: "models/whisper" # 模型下载根目录
    device: "cpu" # 设备

  # FunASR 目前需要在启动时连接互联网以下载/检查模型。您可以在初始化后断开互联网连接。
  # 或者您可以使用 Faster-Whisper 获得完全离线的体验
  fun_asr:
    # "iic/SenseVoiceSmall", "paraformer-zh", "iic/Whisper-large-v3"
    model_name: "iic/SenseVoiceSmall"
    vad_model: "fsmn-vad" # 仅当音频长度超过 30 秒时才需要使用
    punc_model: "ct-punc" # 标点符号模型
    # "cpu"
    device: "mps" # 设备
    disable_update: True # 是否每次启动时都检查 FunASR 更新
    # ncpu: 4 # CPU 内部操作的线程数
    hub: "ms" # ms（默认）从 ModelScope 下载模型。使用 hf 从 Hugging Face 下载模型。
    use_itn: False # 是否使用数字格式转换
    language: "auto" # zh, en, auto

  # pip install sherpa-onnx
  # 文档：https://k2-fsa.github.io/sherpa/onnx/index.html
  # ASR 模型下载：https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
  sherpa_onnx_asr:
    model_type: "sense_voice" # "transducer", "paraformer", "nemo_ctc", "wenet_ctc", "whisper", "tdnn_ctc"
    # 根据 model_type 选择以下其中一个：
    # --- 对于 model_type: "transducer" ---
    # encoder: ""        # 编码器模型路径（例如 "path/to/encoder.onnx"）
    # decoder: ""        # 解码器模型路径（例如 "path/to/decoder.onnx"）
    # joiner: ""         # 连接器模型路径（例如 "path/to/joiner.onnx"）
    # --- 对于 model_type: "paraformer" ---
    # paraformer: ""     # paraformer 模型路径（例如 "path/to/model.onnx"）
    # --- 对于 model_type: "nemo_ctc" ---
    # nemo_ctc: ""        # NeMo CTC 模型路径（例如 "path/to/model.onnx"）
    # --- 对于 model_type: "wenet_ctc" ---
    # wenet_ctc: ""       # WeNet CTC 模型路径（例如 "path/to/model.onnx"）
    # --- 对于 model_type: "tdnn_ctc" ---
    # tdnn_model: ""      # TDNN CTC 模型路径（例如 "path/to/model.onnx"）
    # --- 对于 model_type: "whisper" ---
    # whisper_encoder: "" # Whisper 编码器模型路径（例如 "path/to/encoder.onnx"）
    # whisper_decoder: "" # Whisper 解码器模型路径（例如 "path/to/decoder.onnx"）
    # --- 对于 model_type: "sense_voice" ---
    # SenseVoice 我写了自动下载模型的逻辑，其他模型要自己手动下载
    sense_voice: "./models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx" # SenseVoice 模型路径（例如 "path/to/model.onnx"）
    tokens: "./models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt" # tokens.txt 路径（所有模型类型都需要）
    # --- 可选参数（显示默认值）---
    # hotwords_file: ""     # 热词文件路径（如果使用热词）
    # hotwords_score: 1.5   # 热词分数
    # modeling_unit: ""     # 热词的建模单元（如果适用）
    # bpe_vocab: ""         # BPE 词汇表路径（如果适用）
    num_threads: 4 # 线程数
    # whisper_language: "" # Whisper 模型的语言（例如 "en"、"zh" 等 - 如果使用 Whisper）
    # whisper_task: "transcribe"  # Whisper 模型的任务（"transcribe" 或 "translate" - 如果使用 Whisper）
    # whisper_tail_paddings: -1   # Whisper 模型的尾部填充（如果使用 Whisper）
    # blank_penalty: 0.0    # 空白符号的惩罚
    # decoding_method: "greedy_search"  # "greedy_search" 或 "modified_beam_search"
    # debug: False # 启用调试模式
    # sample_rate: 16000 # 采样率（应与模型预期的采样率匹配）
    # feature_dim: 80       # 特征维度（应与模型预期的特征维度匹配）
    use_itn: True # 对 SenseVoice 模型启用 ITN（如果不是 SenseVoice 模型，则应设置为 False）
    # 推理平台（cpu 或 cuda）(cuda 需要额外配置，请参考文档)
    provider: "coreml"

# =================== Voice Activity Detection ===================
vad_config:
  vad_model: "silero_vad"
  silero_vad:
    orig_sr: 16000 # 原始音频采样率
    target_sr: 16000 # 目标音频采样率
    prob_threshold: 0.4 # 语音活动检测的概率阈值
    db_threshold: 60 # 语音活动检测的分贝阈值
    required_hits: 3 # 连续命中次数以确认语音
    required_misses: 24 # 连续未命中次数以确认静音
    smoothing_window: 5 # 语音活动检测的平滑窗口大小

# speaker_diarization_config:
#   segmentation_model: "./models/sherpa-onnx-pyannote-segmentation-3-0/model.onnx"
#   embedding_extractor_model: "./models/3dspeaker_speech_eres2net_base_sv_zh-cn_3dspeaker_16k.onnx"
#   min_duration_on: 0.3 min_duration_off: 0.5
